{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day2_Tokenization:Sentence.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMOfkmJ9QCLvDy9FkeSNGpj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/psbhargava/100daysofnlp/blob/master/Day2_Tokenization_Sentence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2vK73N31la7",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Sentence Tokenization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaZ00MTI1ZxU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Capture from https://en.wikipedia.org/wiki/Lexical_analysis\n",
        "\n",
        "article1 = 'In computer science, lexical analysis, lexing or tokenization is the process of \\\n",
        "converting a sequence of characters (such as in a computer program or web page) into a \\\n",
        "sequence of tokens (strings with an assigned and thus identified meaning). A program that \\\n",
        "performs lexical analysis may be termed a lexer, tokenizer,[1] or scanner, though scanner \\\n",
        "is also a term for the first stage of a lexer. A lexer is generally combined with a parser, \\\n",
        "which together analyze the syntax of programming languages, web pages, and so forth.'\n",
        "\n",
        "article2 = 'ConcateStringAnd123 ConcateSepcialCharacter_!@# !@#$%^&*()_+ 0123456'\n",
        "\n",
        "article3 = 'It is a great moment from 10 a.m. to 1 p.m. every weekend.'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5qH6jFt1o8M",
        "colab_type": "text"
      },
      "source": [
        "## Self build"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAFcaqZQ1pcS",
        "colab_type": "code",
        "outputId": "6b8dd4f6-5341-4d2e-e1f3-85e6d7d276db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        }
      },
      "source": [
        "import re\n",
        "for doc in [article1, article2, article3]:\n",
        "    print('Original Article: %s' % (doc))\n",
        "    print()\n",
        "\n",
        "    sentences = re.split('(\\.|!|\\?)', doc)\n",
        "    \n",
        "    for i, s in enumerate(sentences):\n",
        "        print('-->Sentence %d: %s' % (i, s))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Article: In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters (such as in a computer program or web page) into a sequence of tokens (strings with an assigned and thus identified meaning). A program that performs lexical analysis may be termed a lexer, tokenizer,[1] or scanner, though scanner is also a term for the first stage of a lexer. A lexer is generally combined with a parser, which together analyze the syntax of programming languages, web pages, and so forth.\n",
            "\n",
            "-->Sentence 0: In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters (such as in a computer program or web page) into a sequence of tokens (strings with an assigned and thus identified meaning)\n",
            "-->Sentence 1: .\n",
            "-->Sentence 2:  A program that performs lexical analysis may be termed a lexer, tokenizer,[1] or scanner, though scanner is also a term for the first stage of a lexer\n",
            "-->Sentence 3: .\n",
            "-->Sentence 4:  A lexer is generally combined with a parser, which together analyze the syntax of programming languages, web pages, and so forth\n",
            "-->Sentence 5: .\n",
            "-->Sentence 6: \n",
            "Original Article: ConcateStringAnd123 ConcateSepcialCharacter_!@# !@#$%^&*()_+ 0123456\n",
            "\n",
            "-->Sentence 0: ConcateStringAnd123 ConcateSepcialCharacter_\n",
            "-->Sentence 1: !\n",
            "-->Sentence 2: @# \n",
            "-->Sentence 3: !\n",
            "-->Sentence 4: @#$%^&*()_+ 0123456\n",
            "Original Article: It is a great moment from 10 a.m. to 1 p.m. every weekend.\n",
            "\n",
            "-->Sentence 0: It is a great moment from 10 a\n",
            "-->Sentence 1: .\n",
            "-->Sentence 2: m\n",
            "-->Sentence 3: .\n",
            "-->Sentence 4:  to 1 p\n",
            "-->Sentence 5: .\n",
            "-->Sentence 6: m\n",
            "-->Sentence 7: .\n",
            "-->Sentence 8:  every weekend\n",
            "-->Sentence 9: .\n",
            "-->Sentence 10: \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re3f196I3IyL",
        "colab_type": "text"
      },
      "source": [
        "You can see that, \"a.m.\" should treat as a \"word\". Of course, we can enhance the above regular expression to do it. But I will go for library rather than build the wheel again"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2TDwSDX3KAa",
        "colab_type": "text"
      },
      "source": [
        "## spaCy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS5wmF8A2Zj7",
        "colab_type": "code",
        "outputId": "a856e8f8-4514-4de1-f4a7-1232906a2f00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import spacy\n",
        "print('spaCy Version: %s' % spacy.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "spaCy Version: 2.1.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJWzKmNs2auj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spacy_nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sm43GJvl2jAi",
        "colab_type": "code",
        "outputId": "7c2ff1a1-882d-4fd1-bffc-733340d44d54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        }
      },
      "source": [
        "for article in [article1, article2, article3]:\n",
        "    print('Original Article: %s' % (article))\n",
        "    print()\n",
        "    doc = spacy_nlp(article)\n",
        "    for i, token in enumerate(doc.sents):\n",
        "        print('-->Sentence %d: %s' % (i, token.text))\n",
        "        print()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Article: In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters (such as in a computer program or web page) into a sequence of tokens (strings with an assigned and thus identified meaning). A program that performs lexical analysis may be termed a lexer, tokenizer,[1] or scanner, though scanner is also a term for the first stage of a lexer. A lexer is generally combined with a parser, which together analyze the syntax of programming languages, web pages, and so forth.\n",
            "\n",
            "-->Sentence 0: In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters (such as in a computer program or web page) into a sequence of tokens (strings with an assigned and thus identified meaning).\n",
            "\n",
            "-->Sentence 1: A program that performs lexical analysis may be termed a lexer, tokenizer,[1] or scanner, though scanner is also a term for the first stage of a lexer.\n",
            "\n",
            "-->Sentence 2: A lexer is generally combined with a parser, which together analyze the syntax of programming languages, web pages, and so forth.\n",
            "\n",
            "Original Article: ConcateStringAnd123 ConcateSepcialCharacter_!@# !@#$%^&*()_+ 0123456\n",
            "\n",
            "-->Sentence 0: ConcateStringAnd123\n",
            "\n",
            "-->Sentence 1: ConcateSepcialCharacter_!@# !@#$%^&*()_+ 0123456\n",
            "\n",
            "Original Article: It is a great moment from 10 a.m. to 1 p.m. every weekend.\n",
            "\n",
            "-->Sentence 0: It is a great moment from 10 a.m. to 1 p.m. every weekend.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44ZNyT3T2l12",
        "colab_type": "text"
      },
      "source": [
        "## NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LPhxCl22m4H",
        "colab_type": "code",
        "outputId": "9c5f532f-b310-4bfd-c9bf-168498161d43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import nltk\n",
        "print('NTLK Version: %s' % nltk.__version__)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NTLK Version: 3.2.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-uaFdr_2old",
        "colab_type": "code",
        "outputId": "c0a04cc9-cb16-43b8-8d9c-98542584b3d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3FbManS2zDH",
        "colab_type": "code",
        "outputId": "9f7018e7-764a-4880-85a8-7b8a62ab4448",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "for article in [article1, article2, article3]:\n",
        "    print('Original Article: %s' % (article))\n",
        "    print()\n",
        "    doc = sent_tokenize(article)\n",
        "    for i, token in enumerate(doc):\n",
        "        print('-->Sentence %d: %s' % (i, token))\n",
        "        print()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Article: In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters (such as in a computer program or web page) into a sequence of tokens (strings with an assigned and thus identified meaning). A program that performs lexical analysis may be termed a lexer, tokenizer,[1] or scanner, though scanner is also a term for the first stage of a lexer. A lexer is generally combined with a parser, which together analyze the syntax of programming languages, web pages, and so forth.\n",
            "\n",
            "-->Sentence 0: In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters (such as in a computer program or web page) into a sequence of tokens (strings with an assigned and thus identified meaning).\n",
            "\n",
            "-->Sentence 1: A program that performs lexical analysis may be termed a lexer, tokenizer,[1] or scanner, though scanner is also a term for the first stage of a lexer.\n",
            "\n",
            "-->Sentence 2: A lexer is generally combined with a parser, which together analyze the syntax of programming languages, web pages, and so forth.\n",
            "\n",
            "Original Article: ConcateStringAnd123 ConcateSepcialCharacter_!@# !@#$%^&*()_+ 0123456\n",
            "\n",
            "-->Sentence 0: ConcateStringAnd123 ConcateSepcialCharacter_!\n",
            "\n",
            "-->Sentence 1: @# !\n",
            "\n",
            "-->Sentence 2: @#$%^&*()_+ 0123456\n",
            "\n",
            "Original Article: It is a great moment from 10 a.m. to 1 p.m. every weekend.\n",
            "\n",
            "-->Sentence 0: It is a great moment from 10 a.m. to 1 p.m. every weekend.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}